/*
 * Copyright (c) 2024, Gnosis Research Center, Illinois Institute of Technology
 * All rights reserved.
 *
 * This file is part of IOWarp Core.
 *
 * Redistribution and use in source and binary forms, with or without
 * modification, are permitted provided that the following conditions are met:
 *
 * 1. Redistributions of source code must retain the above copyright notice,
 *    this list of conditions and the following disclaimer.
 *
 * 2. Redistributions in binary form must reproduce the above copyright notice,
 *    this list of conditions and the following disclaimer in the documentation
 *    and/or other materials provided with the distribution.
 *
 * 3. Neither the name of the copyright holder nor the names of its
 *    contributors may be used to endorse or promote products derived from
 *    this software without specific prior written permission.
 *
 * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
 * AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
 * IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
 * ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE
 * LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR
 * CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF
 * SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS
 * INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN
 * CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)
 * ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
 * POSSIBILITY OF SUCH DAMAGE.
 */

/**
 * test_hdf5_assim.cc - Unit test for ParseOmni API with HDF5 file assimilation
 *
 * This test validates the ParseOmni API with HDF5 format by:
 * 1. Creating a test HDF5 file with multiple datasets
 * 2. Serializing an AssimilationCtx for HDF5 format
 * 3. Calling ParseOmni to discover and transfer datasets to CTE
 * 4. Validating that multiple tags were created (one per dataset)
 * 5. Verifying each tag's metadata and data in CTE
 *
 * Test Strategy:
 * - Tests HDF5 format discovery and multi-dataset handling
 * - Tests hierarchical dataset structure (groups)
 * - Tests various data types (int, double, float)
 * - Tests tensor metadata generation
 * - Tests integration with CTE (tag creation, blob storage)
 *
 * Environment Variables:
 * - INIT_CHIMAERA: If set to "1", initializes Chimaera runtime
 * - TEST_HDF5_FILE: Override default test file path
 */

#include <iostream>
#include <fstream>
#include <vector>
#include <cstdlib>
#include <cstring>
#include <cmath>
#include <memory>
#include <algorithm>

// HDF5 library
#include <hdf5.h>

// Chimaera and CAE headers
#include <chimaera/chimaera.h>
#include <wrp_cae/core/core_client.h>
#include <wrp_cae/core/constants.h>
#include <wrp_cae/core/factory/assimilation_ctx.h>

// CTE headers
#include <wrp_cte/core/core_client.h>

// Logging
#include <hermes_shm/util/logging.h>

// Test configuration
const std::string kTestFileName = "/tmp/test_hdf5_assim_file.h5";
const std::string kTestTagBase = "test_hdf5_tag";

/**
 * Generate a test HDF5 file with multiple datasets
 * This creates a file with various data types and dimensions
 */
bool GenerateTestHDF5File(const std::string& file_path) {
  HLOG(kInfo, "Generating test HDF5 file: {}", file_path);

  // Create HDF5 file
  hid_t file_id = H5Fcreate(file_path.c_str(), H5F_ACC_TRUNC, H5P_DEFAULT, H5P_DEFAULT);
  if (file_id < 0) {
    HLOG(kError, "Failed to create HDF5 file: {}", file_path);
    return false;
  }

  // Dataset 1: /int_dataset - 1D array of 100 integers
  {
    hsize_t dims[1] = {100};
    hid_t dataspace_id = H5Screate_simple(1, dims, NULL);
    hid_t dataset_id = H5Dcreate2(file_id, "/int_dataset", H5T_NATIVE_INT,
                                 dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    std::vector<int> data(100);
    for (int i = 0; i < 100; ++i) {
      data[i] = i * 10;
    }
    H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data.data());

    H5Dclose(dataset_id);
    H5Sclose(dataspace_id);
    HLOG(kInfo, "Created /int_dataset: 1D array of 100 integers");
  }

  // Dataset 2: /double_dataset - 2D array (10x20) of doubles
  {
    hsize_t dims[2] = {10, 20};
    hid_t dataspace_id = H5Screate_simple(2, dims, NULL);
    hid_t dataset_id = H5Dcreate2(file_id, "/double_dataset", H5T_NATIVE_DOUBLE,
                                 dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    std::vector<double> data(200);
    for (int i = 0; i < 200; ++i) {
      data[i] = i * 1.5;
    }
    H5Dwrite(dataset_id, H5T_NATIVE_DOUBLE, H5S_ALL, H5S_ALL, H5P_DEFAULT, data.data());

    H5Dclose(dataset_id);
    H5Sclose(dataspace_id);
    HLOG(kInfo, "Created /double_dataset: 2D array (10x20) of doubles");
  }

  // Dataset 3: /float_dataset - 1D array of 50 floats
  {
    hsize_t dims[1] = {50};
    hid_t dataspace_id = H5Screate_simple(1, dims, NULL);
    hid_t dataset_id = H5Dcreate2(file_id, "/float_dataset", H5T_NATIVE_FLOAT,
                                 dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    std::vector<float> data(50);
    for (int i = 0; i < 50; ++i) {
      data[i] = i * 2.5f;
    }
    H5Dwrite(dataset_id, H5T_NATIVE_FLOAT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data.data());

    H5Dclose(dataset_id);
    H5Sclose(dataspace_id);
    HLOG(kInfo, "Created /float_dataset: 1D array of 50 floats");
  }

  // Dataset 4: /group/nested_dataset - Nested dataset to test hierarchical discovery
  {
    // Create group
    hid_t group_id = H5Gcreate2(file_id, "/group", H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    hsize_t dims[1] = {30};
    hid_t dataspace_id = H5Screate_simple(1, dims, NULL);
    hid_t dataset_id = H5Dcreate2(group_id, "nested_dataset", H5T_NATIVE_INT,
                                 dataspace_id, H5P_DEFAULT, H5P_DEFAULT, H5P_DEFAULT);

    std::vector<int> data(30);
    for (int i = 0; i < 30; ++i) {
      data[i] = i * 5;
    }
    H5Dwrite(dataset_id, H5T_NATIVE_INT, H5S_ALL, H5S_ALL, H5P_DEFAULT, data.data());

    H5Dclose(dataset_id);
    H5Sclose(dataspace_id);
    H5Gclose(group_id);
    HLOG(kInfo, "Created /group/nested_dataset: nested 1D array of 30 integers");
  }

  H5Fclose(file_id);
  HLOG(kSuccess, "Test HDF5 file generated successfully");
  return true;
}

/**
 * Verify dataset data by comparing HDF5 source with CTE tag data
 *
 * @param file_path Path to the HDF5 file
 * @param dataset_path Path to the dataset within the HDF5 file
 * @param tag_name Full tag name in CTE
 * @param cte_client Pointer to CTE client
 * @return true if data matches, false otherwise
 */
bool VerifyDatasetData(const std::string& file_path,
                       const std::string& dataset_path,
                       const std::string& tag_name,
                       wrp_cte::core::Client* cte_client) {
  HLOG(kInfo, "Verifying data for dataset: {}", dataset_path);

  // Open HDF5 file and dataset
  hid_t file_id = H5Fopen(file_path.c_str(), H5F_ACC_RDONLY, H5P_DEFAULT);
  if (file_id < 0) {
    HLOG(kError, "Failed to open HDF5 file: {}", file_path);
    return false;
  }

  hid_t dataset_id = H5Dopen2(file_id, dataset_path.c_str(), H5P_DEFAULT);
  if (dataset_id < 0) {
    HLOG(kError, "Failed to open dataset: {}", dataset_path);
    H5Fclose(file_id);
    return false;
  }

  // Get dataset properties
  hid_t dataspace_id = H5Dget_space(dataset_id);
  hid_t datatype_id = H5Dget_type(dataset_id);

  hssize_t num_elements = H5Sget_simple_extent_npoints(dataspace_id);
  if (num_elements < 0) {
    HLOG(kError, "Failed to get number of elements");
    H5Tclose(datatype_id);
    H5Sclose(dataspace_id);
    H5Dclose(dataset_id);
    H5Fclose(file_id);
    return false;
  }

  size_t element_size = H5Tget_size(datatype_id);
  size_t total_size = num_elements * element_size;

  HLOG(kInfo, "Dataset info: {} elements, {} bytes per element, {} total bytes",
       num_elements, element_size, total_size);

  // Allocate buffer for HDF5 data
  std::vector<char> hdf5_data(total_size);

  // Read data from HDF5
  herr_t status = H5Dread(dataset_id, datatype_id, H5S_ALL, H5S_ALL, H5P_DEFAULT, hdf5_data.data());
  if (status < 0) {
    HLOG(kError, "Failed to read data from HDF5 dataset");
    H5Tclose(datatype_id);
    H5Sclose(dataspace_id);
    H5Dclose(dataset_id);
    H5Fclose(file_id);
    return false;
  }

  // Get CTE tag
  auto tag_task = cte_client->AsyncGetOrCreateTag(tag_name);
  tag_task.Wait();
  wrp_cte::core::TagId tag_id = tag_task->tag_id_;
  if (tag_id.IsNull()) {
    HLOG(kError, "Tag not found in CTE: {}", tag_name);
    H5Tclose(datatype_id);
    H5Sclose(dataspace_id);
    H5Dclose(dataset_id);
    H5Fclose(file_id);
    return false;
  }

  // Read data from CTE by getting all blobs (chunks)
  // For datasets <= 1MB, data is in "chunk_0"
  // For larger datasets, data is split across "chunk_0", "chunk_1", etc.
  auto blobs_task = cte_client->AsyncGetContainedBlobs(tag_id);
  blobs_task.Wait();
  std::vector<std::string> blob_names = blobs_task->blob_names_;
  HLOG(kInfo, "Found {} blobs in tag", blob_names.size());

  // Filter out the "description" blob and get only chunk blobs
  std::vector<std::string> chunk_blobs;
  for (const auto& blob_name : blob_names) {
    if (blob_name.find("chunk_") == 0) {
      chunk_blobs.push_back(blob_name);
    }
  }

  // Sort chunk blobs by number to ensure correct order
  std::sort(chunk_blobs.begin(), chunk_blobs.end(), [](const std::string& a, const std::string& b) {
    // Extract chunk numbers and compare
    size_t a_num = std::stoul(a.substr(6)); // Skip "chunk_"
    size_t b_num = std::stoul(b.substr(6));
    return a_num < b_num;
  });

  HLOG(kInfo, "Found {} data chunks", chunk_blobs.size());

  // Calculate total chunk data size (excludes metadata blobs like "description")
  size_t total_chunk_size = 0;
  for (const auto& blob_name : chunk_blobs) {
    auto blob_size_task = cte_client->AsyncGetBlobSize(tag_id, blob_name);
    blob_size_task.Wait();
    total_chunk_size += blob_size_task->size_;
  }
  HLOG(kInfo, "Total chunk data size: {} bytes", total_chunk_size);

  // Check if chunk data sizes match (ignoring metadata blobs)
  if (total_chunk_size != total_size) {
    HLOG(kError, "Size mismatch - HDF5: {} bytes, CTE chunks: {} bytes",
         total_size, total_chunk_size);
    H5Tclose(datatype_id);
    H5Sclose(dataspace_id);
    H5Dclose(dataset_id);
    H5Fclose(file_id);
    return false;
  }

  // Allocate buffer for CTE data
  std::vector<char> cte_data(total_size);

  // Read all chunks and reconstruct data
  size_t bytes_read = 0;
  for (const auto& blob_name : chunk_blobs) {
    // Get blob size
    auto blob_size_task = cte_client->AsyncGetBlobSize(tag_id, blob_name);
    blob_size_task.Wait();
    chi::u64 blob_size = blob_size_task->size_;
    HLOG(kInfo, "Reading blob '{}' (size: {} bytes)", blob_name, blob_size);

    if (bytes_read + blob_size > total_size) {
      HLOG(kError, "Total blob size exceeds expected size");
      H5Tclose(datatype_id);
      H5Sclose(dataspace_id);
      H5Dclose(dataset_id);
      H5Fclose(file_id);
      return false;
    }

    // Allocate shared memory buffer for this blob
    auto blob_buffer = CHI_IPC->AllocateBuffer(blob_size);

    // Read blob into shared memory buffer
    hipc::ShmPtr<> blob_shm_ptr = blob_buffer.shm_.template Cast<void>();
    auto get_blob_task = cte_client->AsyncGetBlob(tag_id, blob_name, 0, blob_size, 0, blob_shm_ptr);
    get_blob_task.Wait();
    bool success = (get_blob_task->GetReturnCode() == 0);
    if (!success) {
      HLOG(kError, "Failed to read blob '{}'", blob_name);
      CHI_IPC->FreeBuffer(blob_buffer);
      H5Tclose(datatype_id);
      H5Sclose(dataspace_id);
      H5Dclose(dataset_id);
      H5Fclose(file_id);
      return false;
    }

    // Copy from shared memory to our local buffer
    std::memcpy(cte_data.data() + bytes_read, blob_buffer.ptr_, blob_size);

    // Free the shared memory buffer
    CHI_IPC->FreeBuffer(blob_buffer);

    bytes_read += blob_size;
  }

  if (bytes_read != total_size) {
    HLOG(kError, "Failed to read complete data from CTE - expected {} bytes, got {} bytes",
         total_size, bytes_read);
    H5Tclose(datatype_id);
    H5Sclose(dataspace_id);
    H5Dclose(dataset_id);
    H5Fclose(file_id);
    return false;
  }

  HLOG(kSuccess, "Successfully read {} bytes from CTE", bytes_read);

  // Determine data type for comparison
  H5T_class_t type_class = H5Tget_class(datatype_id);
  bool data_matches = true;
  size_t mismatch_count = 0;

  if (type_class == H5T_INTEGER) {
    // Integer comparison - byte-by-byte
    HLOG(kInfo, "Comparing integer data...");
    for (size_t i = 0; i < total_size; ++i) {
      if (hdf5_data[i] != cte_data[i]) {
        if (mismatch_count == 0) {
          HLOG(kError, "First mismatch at byte {}: HDF5={}, CTE={}",
               i, static_cast<int>(hdf5_data[i]), static_cast<int>(cte_data[i]));
        }
        mismatch_count++;
        data_matches = false;
      }
    }
  } else if (type_class == H5T_FLOAT) {
    // Floating-point comparison with epsilon
    if (element_size == sizeof(double)) {
      HLOG(kInfo, "Comparing double data (epsilon=1e-10)...");
      const double* hdf5_doubles = reinterpret_cast<const double*>(hdf5_data.data());
      const double* cte_doubles = reinterpret_cast<const double*>(cte_data.data());
      const double epsilon = 1e-10;

      for (size_t i = 0; i < static_cast<size_t>(num_elements); ++i) {
        double diff = std::abs(hdf5_doubles[i] - cte_doubles[i]);
        if (diff > epsilon) {
          if (mismatch_count == 0) {
            HLOG(kError, "First mismatch at element {}: HDF5={}, CTE={}, diff={}",
                 i, hdf5_doubles[i], cte_doubles[i], diff);
          }
          mismatch_count++;
          data_matches = false;
        }
      }
    } else if (element_size == sizeof(float)) {
      HLOG(kInfo, "Comparing float data (epsilon=1e-6)...");
      const float* hdf5_floats = reinterpret_cast<const float*>(hdf5_data.data());
      const float* cte_floats = reinterpret_cast<const float*>(cte_data.data());
      const float epsilon = 1e-6f;

      for (size_t i = 0; i < static_cast<size_t>(num_elements); ++i) {
        float diff = std::abs(hdf5_floats[i] - cte_floats[i]);
        if (diff > epsilon) {
          if (mismatch_count == 0) {
            HLOG(kError, "First mismatch at element {}: HDF5={}, CTE={}, diff={}",
                 i, hdf5_floats[i], cte_floats[i], diff);
          }
          mismatch_count++;
          data_matches = false;
        }
      }
    } else {
      HLOG(kWarning, "Unsupported float size: {} bytes", element_size);
      // Fall back to byte-by-byte comparison
      for (size_t i = 0; i < total_size; ++i) {
        if (hdf5_data[i] != cte_data[i]) {
          mismatch_count++;
          data_matches = false;
        }
      }
    }
  } else {
    // Unknown type - byte-by-byte comparison
    HLOG(kInfo, "Comparing as raw bytes...");
    for (size_t i = 0; i < total_size; ++i) {
      if (hdf5_data[i] != cte_data[i]) {
        if (mismatch_count == 0) {
          HLOG(kError, "First mismatch at byte {}: HDF5={}, CTE={}",
               i, static_cast<int>(hdf5_data[i]), static_cast<int>(cte_data[i]));
        }
        mismatch_count++;
        data_matches = false;
      }
    }
  }

  // Cleanup HDF5 resources
  H5Tclose(datatype_id);
  H5Sclose(dataspace_id);
  H5Dclose(dataset_id);
  H5Fclose(file_id);

  // Print comparison results
  if (data_matches) {
    HLOG(kSuccess, "Data verification passed - all values match");
  } else {
    HLOG(kError, "Data verification failed - {} mismatches out of {} bytes",
         mismatch_count, total_size);
  }

  return data_matches;
}

/**
 * Clean up test file
 */
void CleanupTestFile(const std::string& file_path) {
  if (std::remove(file_path.c_str()) == 0) {
    HLOG(kInfo, "Test file cleaned up: {}", file_path);
  } else {
    HLOG(kWarning, "Failed to remove test file: {}", file_path);
  }
}

/**
 * Main test function
 */
int main(int argc, char* argv[]) {
  HLOG(kInfo, "======================================");
  HLOG(kInfo, "HDF5 Assimilation ParseOmni Unit Test");
  HLOG(kInfo, "======================================");

  int exit_code = 0;

  try {
    // Initialize Chimaera runtime (CHI_WITH_RUNTIME controls behavior)
    HLOG(kInfo, "Initializing Chimaera...");
    bool success = chi::CHIMAERA_INIT(chi::ChimaeraMode::kClient, true);
    if (!success) {
      HLOG(kError, "Failed to initialize Chimaera");
      return 1;
    }
    HLOG(kSuccess, "Chimaera initialized successfully");

    // Verify Chimaera IPC is available
    auto* ipc_manager = CHI_IPC;
    if (!ipc_manager) {
      HLOG(kError, "Chimaera IPC not initialized");
      return 1;
    }
    HLOG(kSuccess, "Chimaera IPC verified");

    // Step 1: Generate test HDF5 file
    HLOG(kInfo, "[STEP 1] Generating test HDF5 file...");
    if (!GenerateTestHDF5File(kTestFileName)) {
      return 1;
    }

    // Step 2: Connect to CTE
    HLOG(kInfo, "[STEP 2] Connecting to CTE...");
    wrp_cte::core::WRP_CTE_CLIENT_INIT();
    HLOG(kSuccess, "CTE client initialized");

    // Step 2.5: Initialize CAE client
    HLOG(kInfo, "[STEP 2.5] Initializing CAE client...");
    WRP_CAE_CLIENT_INIT();
    HLOG(kSuccess, "CAE client initialized");

    // Step 3: Create CAE pool
    HLOG(kInfo, "[STEP 3] Creating CAE pool...");
    wrp_cae::core::Client cae_client;
    wrp_cae::core::CreateParams params;

    auto create_task = cae_client.AsyncCreate(
        chi::PoolQuery::Local(),
        "test_cae_pool",
        wrp_cae::core::kCaePoolId,
        params);
    create_task.Wait();

    HLOG(kSuccess, "CAE pool created with ID: {}", cae_client.pool_id_);

    // Step 4: Create AssimilationCtx for HDF5
    HLOG(kInfo, "[STEP 4] Creating AssimilationCtx for HDF5...");
    wrp_cae::core::AssimilationCtx ctx;
    ctx.src = "hdf5::" + kTestFileName;
    ctx.dst = "iowarp::" + kTestTagBase;
    ctx.format = "hdf5";
    ctx.depends_on = "";
    ctx.range_off = 0;
    ctx.range_size = 0;  // 0 means process entire file

    HLOG(kInfo, "AssimilationCtx created:");
    HLOG(kInfo, "  src: {}", ctx.src);
    HLOG(kInfo, "  dst: {}", ctx.dst);
    HLOG(kInfo, "  format: {}", ctx.format);

    // Step 5: Call ParseOmni with vector containing single context
    HLOG(kInfo, "[STEP 5] Calling ParseOmni...");
    std::vector<wrp_cae::core::AssimilationCtx> contexts = {ctx};
    auto parse_task = cae_client.AsyncParseOmni(contexts);
    parse_task.Wait();
    chi::u32 result_code = parse_task->GetReturnCode();
    chi::u32 num_tasks_scheduled = parse_task->num_tasks_scheduled_;

    HLOG(kInfo, "ParseOmni completed:");
    HLOG(kInfo, "  result_code: {}", result_code);
    HLOG(kInfo, "  num_tasks_scheduled: {}", num_tasks_scheduled);

    // Step 6: Validate results
    HLOG(kInfo, "[STEP 6] Validating results...");

    if (result_code != 0) {
      HLOG(kError, "ParseOmni failed with result_code: {}", result_code);
      exit_code = 1;
    } else if (num_tasks_scheduled == 0) {
      HLOG(kError, "ParseOmni returned 0 tasks scheduled");
      exit_code = 1;
    } else {
      HLOG(kSuccess, "ParseOmni executed successfully");
    }

    // Step 7: Verify datasets in CTE
    HLOG(kInfo, "[STEP 7] Verifying datasets in CTE...");

    // Get CTE client
    auto cte_client = WRP_CTE_CLIENT;

    // Expected dataset names (based on HDF5 file structure)
    std::vector<std::string> expected_datasets = {
      "int_dataset",
      "double_dataset",
      "float_dataset",
      "group/nested_dataset"
    };

    HLOG(kInfo, "Expected {} datasets to be created", expected_datasets.size());

    size_t datasets_found = 0;
    size_t datasets_verified = 0;
    for (const auto& dataset_name : expected_datasets) {
      std::string full_tag_name = kTestTagBase + "/" + dataset_name;
      std::string dataset_path = "/" + dataset_name;
      HLOG(kInfo, "Checking dataset: {}", dataset_name);
      HLOG(kInfo, "  Full tag name: {}", full_tag_name);

      // Check if tag exists
      auto tag_task = cte_client->AsyncGetOrCreateTag(full_tag_name);
      tag_task.Wait();
      wrp_cte::core::TagId tag_id = tag_task->tag_id_;
      if (tag_id.IsNull()) {
        HLOG(kWarning, "Tag not found in CTE: {}", full_tag_name);
        continue;
      }

      datasets_found++;
      HLOG(kSuccess, "Tag found (ID: {})", tag_id);

      // Get tag size
      auto size_task = cte_client->AsyncGetTagSize(tag_id);
      size_task.Wait();
      size_t tag_size = size_task->tag_size_;
      HLOG(kInfo, "  Tag size: {} bytes", tag_size);

      if (tag_size == 0) {
        HLOG(kWarning, "Tag size is 0, no data transferred");
        continue;
      }

      // Verify dataset data by comparing with original HDF5 data
      bool data_verified = VerifyDatasetData(kTestFileName, dataset_path, full_tag_name, cte_client);
      if (data_verified) {
        datasets_verified++;
      } else {
        HLOG(kError, "Data verification failed for dataset: {}", dataset_name);
        exit_code = 1;
      }
    }

    HLOG(kInfo, "Dataset verification summary:");
    HLOG(kInfo, "  Expected datasets: {}", expected_datasets.size());
    HLOG(kInfo, "  Found datasets: {}", datasets_found);
    HLOG(kInfo, "  Verified datasets: {}", datasets_verified);

    if (datasets_found == 0) {
      HLOG(kError, "No datasets found in CTE");
      HLOG(kInfo, "NOTE: HDF5 assimilator may not yet be fully implemented");
      exit_code = 1;
    } else if (datasets_found < expected_datasets.size()) {
      HLOG(kWarning, "Not all datasets were found ({}/{})", datasets_found,
           expected_datasets.size());
      // Not a hard failure - HDF5 assimilator may be under development
    } else if (datasets_verified < datasets_found) {
      HLOG(kError, "Not all datasets passed data verification ({}/{})",
           datasets_verified, datasets_found);
      exit_code = 1;
    } else {
      HLOG(kSuccess, "All expected datasets found and verified in CTE");
    }

    // Step 8: Cleanup
    HLOG(kInfo, "[STEP 8] Cleaning up...");
    CleanupTestFile(kTestFileName);

  } catch (const std::exception& e) {
    HLOG(kError, "Exception caught: {}", e.what());
    exit_code = 1;
  }

  // Print final result
  HLOG(kInfo, "========================================");
  if (exit_code == 0) {
    HLOG(kSuccess, "TEST PASSED");
  } else {
    HLOG(kError, "TEST FAILED");
  }
  HLOG(kInfo, "========================================");

  return exit_code;
}
